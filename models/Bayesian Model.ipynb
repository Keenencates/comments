{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can Emoticons Be Use To Predict Sentiment\n",
    "## Keenen Cates\n",
    "### Bayesian Benchmark Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "We will first load all of the preprocessed data, and split the validation set off. Also, we will make a list of skin tone modifiers for emoticons the obfuscate the prediction (i.e. a skin tone modifier can accompany many emoticons to modify their skin tone). A deeper look will need to be done into how to deal with multi-colored hearts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "from os import path\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from emoji_list import all_emoji\n",
    "from time import strftime\n",
    "from urllib.request import urlopen \n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import helper\n",
    "import math\n",
    "\n",
    "#Load comments, targets, and dictionaries for embeddings, and holdout validation data\n",
    "content, targets, emoji_to_int, int_to_emoji, vocab_to_int, int_to_vocab, token_lookup = helper.load_preprocess()\n",
    "X, Y, VAL_X, VAL_Y = helper.peel_validation(content, targets)\n",
    "skin_tones = helper.skin_tone_emojis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "The first step is to build our model of prior probabilities for each class and probabilities of each class relative to a given word. We could likely extend our model by changing the level at which we are modelling (i.e. N-Gram models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_naive_bayes_model(model_x, model_y):\n",
    "    \"\"\" Takes a corpus of embedded comments and labels \n",
    "        and returns Naive Bayes Probabilities\n",
    "    :type model_x: List[List[Int]]\n",
    "    :type model_y: List[List[Int]]\n",
    "    :rtype       : Int, Dict, Dict, Dict, Dict\n",
    "    \"\"\"\n",
    "    n_texts = 0\n",
    "    class_dict = {emoji_to_int[k]:Counter() for k in all_emoji if k in emoji_to_int}\n",
    "    n_texts_class = {emoji_to_int[k]:1 for k in all_emoji if k in emoji_to_int}\n",
    "    \n",
    "    for i in range(len(model_x)):\n",
    "        n_texts += 1\n",
    "        for each in model_y[i]:\n",
    "            class_dict[each].update(model_x[i])\n",
    "            n_texts_class[each] += 1\n",
    "\n",
    "    rel_freq_class_dict = {emoji_to_int[k]:dict() for k in all_emoji if k in emoji_to_int}\n",
    "    \n",
    "    for class_key, counts in class_dict.items():\n",
    "        total_words = sum([val for _, val in class_dict[class_key].items()])\n",
    "        for word in counts:\n",
    "            rel_freq_class_dict[class_key][word] = class_dict[class_key][word] / total_words \n",
    "\n",
    "    prior_prob_class = {emoji_to_int[k]:n_texts_class[emoji_to_int[k]] / n_texts  for k in all_emoji if k in emoji_to_int}\n",
    "    \n",
    "    return n_texts, class_dict, n_texts_class, rel_freq_class_dict, prior_prob_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_texts, class_dict, n_texts_class, rel_freq_class_dict, prior_prob_class = generate_naive_bayes_model(X, Y)\n",
    "\n",
    "def get_rel_freq(class_key, word):\n",
    "    \"\"\"Takes a class and a word and return prob\n",
    "       of the class given the word.\n",
    "    :type class_key: Int\n",
    "    :type word     : Int\n",
    "    :rtype         : Float\n",
    "    \"\"\"\n",
    "    if word in rel_freq_class_dict[class_key]:\n",
    "        return rel_freq_class_dict[class_key][word]\n",
    "    else:\n",
    "        return .000000000001\n",
    "\n",
    "def compute_prob(class_key, text, scale = 1.0):\n",
    "    \"\"\"Computes the probability of a text having a given emoji\n",
    "    :type class_key: Int\n",
    "    :type text     : List[Int]\n",
    "    :type scale    : Float\n",
    "    :rtype         : Float\n",
    "    \"\"\"\n",
    "    prod_freqs = np.prod([get_rel_freq(class_key, word) for word in text])\n",
    "    prior = prior_prob_class[class_key]\n",
    "    return prod_freqs * (prior * scale)\n",
    "\n",
    "def create_probs(text, scale = 1.0):\n",
    "    \"\"\"Computes the probability of all classes for a given text\n",
    "    :type text     : List[Int]\n",
    "    :type scale    : Float\n",
    "    :rtype         : Float\n",
    "    \"\"\"\n",
    "    return {emoji_to_int[k]:compute_prob(emoji_to_int[k], text, scale) for k in all_emoji if k in emoji_to_int}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "We will make predictions make selecting the top k most likely classes. This is because there is some ambiguity to emoticons, i.e. there are multiple types of hearts and smiley faces with hearts. That means that multiple classes could be appropriate for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_k_predictions(text, k, preprocess=False, scale = 1.0):\n",
    "    \"\"\"Computes the top k most probable classes\n",
    "    :type text      : List[Int]\n",
    "    :type k         : Int\n",
    "    :type preprocess: Boolean\n",
    "    :type scale     : Float\n",
    "    :rtype          : List[Int]\n",
    "    \"\"\"\n",
    "    if preprocess:\n",
    "        text = helper.tokenize_and_embed(text, token_lookup, vocab_to_int)\n",
    "    probs = create_probs(text, scale)\n",
    "    probs = {k:v for k, v in probs.items()}\n",
    "    probs = probs.items()\n",
    "    sorted_probs = sorted(probs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    res = []\n",
    "    idx = 0\n",
    "    while len(res) < k:\n",
    "        #We want to ignore the skin tone modifiers\n",
    "        if int_to_emoji[sorted_probs[idx][0]] not in skin_tones:\n",
    "            res.append(sorted_probs[idx][0])\n",
    "        idx += 1\n",
    "    return res\n",
    "\n",
    "def dummy_top_k_predict(x, k):\n",
    "    \"\"\"Predicts the top k most frequent classes\n",
    "    :type x: List[Int]\n",
    "    :type k: Int\n",
    "    :rtype : List[Int]\n",
    "    \"\"\"\n",
    "    classes = prior_prob_class.items()\n",
    "    classes = sorted(classes, key = lambda x: x[1], reverse = True)\n",
    "    return [classes[i][0] for i in range(0, k)]\n",
    "\n",
    "def print_nth_prediction(text_n, scale=1.0):\n",
    "    \"\"\"Prints a nicely formatted chart for comparing results\n",
    "    :type text      : List[Int]\n",
    "    :type scale     : Float\n",
    "    \"\"\"\n",
    "    print('Comment     :', helper.get_nth_text(content, int_to_vocab, text_n))\n",
    "    print('Emoticons   :', helper.get_nth_label(targets, int_to_emoji, text_n))\n",
    "    print('Top 5 Preds :', ' '.join(int_to_emoji[each] for each in top_k_predictions(content[text_n], 5, scale = scale)))\n",
    "    print('Dummy Pred  :', ' '.join(int_to_emoji[each] for each in dummy_top_k_predict(content[text_n], 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "Here are some examples of the Models Prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment     : logan paul it's yo big day\n",
      "Emoticons   : ‼️\n",
      "Top 5 Preds : ‼ 😂 💯 😄 😁\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : evan is being a douch logans getting pissed <comma_sign>  logan should have went to smash his game system then evan would have cried  & evans not gonna be able to take care of a pitbull tell his ass no <comma_sign>  its gonna kill kong  <exclamation_mark>  <exclamation_mark>  <exclamation_mark>\n",
      "Emoticons   : 😂\n",
      "Top 5 Preds : 😂 😍 ️ 😭 ❤\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : me and my friends subscribe aaaaaaaaaaaa you got the diamond play button\n",
      "Emoticons   : 💎😆😁\n",
      "Top 5 Preds : 💎 😁 😆 ❣ 😚\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : saying nigga isn't even bad it's only bad if you say it in a hateful way to black people all the cry babies and immature people stop going at him i guarantee youse you wouldn't say it to idubbz or others but because it's pewdiepie you think you have the right to go full force at him yet i can see were people are coming from because pewdiepie did start the biggest ad crisis on any platform ever he has to much pressure on  him that's my speech down  btw i do like idubbz it was just an\n",
      "Emoticons   : 😂\n",
      "Top 5 Preds : 😂 ❤ 😍 ️ 😭\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : just wait  <period>  <period>  <period>  i'll sell my kidney n bay it  <period>\n",
      "Emoticons   : 💔😂\n",
      "Top 5 Preds : 💔 😂 👏 😀 😭\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     :  watch by clicking here you can see people's are\n",
      "Emoticons   : 👈\n",
      "Top 5 Preds : 👈 ️ 😂 😇 😓\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : it has 2 colors u cant afford it\n",
      "Emoticons   : 😂\n",
      "Top 5 Preds : 💜 😂 😭 ❤ 😅\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     :  watch by clicking here you can see people's are\n",
      "Emoticons   : 👈\n",
      "Top 5 Preds : 👈 ️ 😂 😇 😓\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : trump sees hud primarily as a tool for enriching well-connected real-estate developers <comma_sign>  carson’s combination of loyalty and ignorance may make him uniquely “qualified” for the position <period> \n",
      "Emoticons   : 😠\n",
      "Top 5 Preds : 😠 😂 ❤ ️ 😉\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : how could ben carson cou#$ dfj$%  <period>  <period>  <period>  <period>  <period>  <period>  <period> \n",
      "Emoticons   : 😴\n",
      "Top 5 Preds : 😴 😂 💏 😐 🤗\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : lol\n",
      "Emoticons   : 😂\n",
      "Top 5 Preds : 😂 ❤ ️ 😍 🤣\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : love it <exclamation_mark>  i've never thought as roman hot <comma_sign>  but i'm digging the white hat on him <period>  he looks hot <exclamation_mark>  lolol now is country single <question_mark> \n",
      "Emoticons   : ☺️\n",
      "Top 5 Preds : 😂 ☺ 👍 ️ 👌\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : thanks so much for inspiring me to make my own channel and vids  <return> notifications on and subscribed <return> love u\n",
      "Emoticons   : 😄\n",
      "Top 5 Preds : 😄 ❤ ️ 🙏 😉\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : cora is so cute but i can't wait untill she speakswho else <question_mark>\n",
      "Emoticons   : 😊\n",
      "Top 5 Preds : 😊 ❤ 😭 😍 😂\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : say how do you like dem apples    <question_mark>\n",
      "Emoticons   : 🍏🍎\n",
      "Top 5 Preds : 🍏 🍎 ️ ❤ 😂\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : don't forget paying hundreds of dollars for a parking permit and not being able to find parking\n",
      "Emoticons   : ‍♀🤦🏽️\n",
      "Top 5 Preds : 🤦 ♀ ‍ ️ 😂\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : this was the funniest interview\n",
      "Emoticons   : 😂️✌\n",
      "Top 5 Preds : 😂 ️ ❤ 😍 😅\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : having 1 is to close to having none  my nigga mac dreweather definitely using that next time i get caught\n",
      "Emoticons   : 😂\n",
      "Top 5 Preds : 😂 😭 ️ ‍ 🤔\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : why doesn't this account have at least 3m subscribers <question_mark>  <exclamation_mark> \n",
      "Emoticons   : 😱❤️\n",
      "Top 5 Preds : 😱 😂 ️ ❤ 💕\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n",
      "Comment     : tmt = too many tacos       <return> you rock diego mancuso\n",
      "Emoticons   : 🌮😂\n",
      "Top 5 Preds : 🌮 😂 ️ 😀 ❤\n",
      "Dummy Pred  : 😂 ❤ 😍 ️ 😭\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100, 5):\n",
    "    print_nth_prediction(i)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Finally we test our model! It appears that the model is having some problems generalizing. I am inclined to think the N-gram models could help us improve our generality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Training Accuracy : 0.8587166574979194\n"
     ]
    }
   ],
   "source": [
    "print('Bayesian Training Accuracy :', helper.top_k_categorical_accuracy(X, Y, top_k_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Training Accuracy    : 0.5265258911317056\n"
     ]
    }
   ],
   "source": [
    "print('Dummy Training Accuracy    :', helper.top_k_categorical_accuracy(X, Y, dummy_top_k_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Validation Accuracy : 0.5476754682915821\n",
      "Dummy Validation Accuracy    : 0.5265177160911758\n"
     ]
    }
   ],
   "source": [
    "print('Bayesian Validation Accuracy :', helper.top_k_categorical_accuracy(VAL_X, VAL_Y, top_k_predictions))\n",
    "print('Dummy Validation Accuracy    :', helper.top_k_categorical_accuracy(VAL_X, VAL_Y, dummy_top_k_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
