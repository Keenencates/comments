{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "Keenen Cates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "from os import path\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from emoji_list import all_emoji\n",
    "from time import strftime\n",
    "from urllib.request import urlopen \n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import pickle\n",
    "\n",
    "excluded = ['1','2','3','4', \n",
    "            '5','6','7','8', \n",
    "            '9','0','#','*']\n",
    "\n",
    "emojis = [e for e in all_emoji if e not in excluded]\n",
    "\n",
    "today = strftime('%x').replace('/', '_')\n",
    "\n",
    "data_url = 'https://www.kaggle.com/datasnaek/youtube/downloads/youtube.zip'\n",
    "root_dir = '.'\n",
    "data_root = path.join(root_dir, 'data')\n",
    "data_path = path.join(data_root, 'data_' + today + '/')\n",
    "test_data = data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments(path):\n",
    "    \"\"\"Takes a path to a data folder \n",
    "       and returns the comments within\n",
    "    :type path: String\n",
    "    :rtype    : DataFrame  \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path + '/UScomments.csv', error_bad_lines=False)\n",
    "    comments = df['comment_text']\n",
    "    \n",
    "    words = Counter()\n",
    "    for each in comments:\n",
    "        words.update(''.join([c for c in str(each).lower() if c not in punctuation]).split(' '))\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_emoji_data(comments, threshold):\n",
    "    \"\"\"Extracts the data that has emoticons\n",
    "    :type comments: DataFrame\n",
    "    :rtype    : DataFrame , DataFrame \n",
    "    \"\"\"\n",
    "    has_emoji = lambda s: any((True for x in [c for c in str(s)] if x in emojis))   \n",
    "    emoji_comments = []\n",
    "    no_emoji_comments = []\n",
    "    for each in comments[:threshold]:\n",
    "        if has_emoji(each):\n",
    "            emoji_comments.append(each)\n",
    "        else:\n",
    "            no_emoji_comments.append(each)\n",
    "            \n",
    "    return emoji_comments, no_emoji_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_emoji_target(emoji_comments):\n",
    "    \"\"\"Extracts the emoticons as labels\n",
    "    :type emoji_comments: DataFrame\n",
    "    :rtype    : List[String], List[String]\n",
    "    \"\"\"\n",
    "    inputs = [] \n",
    "    targets = []\n",
    "    for comment in emoji_comments:\n",
    "        x = []\n",
    "        y = []\n",
    "        for c in comment:\n",
    "            if c in emojis:\n",
    "                y.append(c)\n",
    "            else:\n",
    "                x.append(c)\n",
    "        inputs.append(''.join(x))\n",
    "        targets.append(''.join(set(y)))\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"Generates token lookup table\n",
    "    :rtype    : Dict\n",
    "    \"\"\"\n",
    "    lookup = {}\n",
    "    lookup['!'] = '<EXCLAMATION_MARK>'\n",
    "    lookup['\"'] = '<QOUTATION_MARK>'\n",
    "    lookup['('] = '<LEFT_PARANTHESES>'\n",
    "    lookup[')'] = '<RIGHT_PARANTHESES>'\n",
    "    lookup[','] = '<COMMA_SIGN>'\n",
    "    lookup['.'] = '<PERIOD>'\n",
    "    lookup['--'] = '<DASH>'\n",
    "    lookup[';'] = '<SEMICOLON>'\n",
    "    lookup['?'] = '<QUESTION_MARK>'\n",
    "    lookup['\\\\n'] ='<RETURN>'\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"Generate Embedding tables for words\n",
    "    :type text: List\n",
    "    :rtype    : Dict, Dict\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for comment in text:\n",
    "        word_counts.update(comment)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(inputs, targets):\n",
    "    \"\"\"Create tables and preprocess all the data\n",
    "    :type text: List, List\n",
    "    \"\"\"\n",
    "    token_dict = token_lookup()\n",
    "    pre_comments = []\n",
    "    for comment in inputs:\n",
    "        s = comment\n",
    "        for key, token in token_dict.items():\n",
    "            s = (s.replace(key, ' {} '.format(token)))\n",
    "        pre_comments.append(s)\n",
    "        \n",
    "    comments_s = []\n",
    "    comments_i = []\n",
    "    \n",
    "    targets_s = [list(each) for each in labels]\n",
    "    targets_i = []\n",
    "    \n",
    "    for comment in pre_comments:\n",
    "        comments_s.append(comment.lower().split(' ')[:-1])\n",
    "        \n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(comments_s)\n",
    "    emoji_to_int, int_to_emoji = create_lookup_tables(targets_s)\n",
    "    for comment in comments_s:\n",
    "        int_text = [vocab_to_int[word] for word in comment]\n",
    "        comments_i.append(int_text)\n",
    "    for emoji in targets_s:\n",
    "        int_emoji = [emoji_to_int[e] for e in emoji]\n",
    "        targets_i.append(int_emoji)\n",
    "    pickle.dump((comments_i, targets_i, emoji_to_int, int_to_emoji, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
    "    \n",
    "def load_preprocess():\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    comments = get_comments(test_data)\n",
    "    emoji_comments, no_emoji_comments = split_emoji_data(comments, len(comments))\n",
    "    inputs, labels = extract_emoji_target(emoji_comments)\n",
    "    preprocess_and_save_data(inputs, labels)\n",
    "    pickle.dump((no_emoji_comments), open('no_emojis.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
